---
title: "Model Fitting, Bias, & Variance: Solutions"
author: "Philip Waggoner, MACS 30100 <br /> University of Chicago"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, warning = FALSE)

library(tidyverse)
library(here)

set.seed(1234)
theme_set(theme_minimal())
```

$$\newcommand{\E}{\mathrm{E}} \newcommand{\Var}{\mathrm{Var}} \newcommand{\Cov}{\mathrm{Cov}} \newcommand{\loglik}{\text{logLik}}$$

# On your own

For this section, you will work in small groups of 4-5. *I will create these groups at random*. 

**IMPORTANT**: _Don't forget that this code you're working on here is due at the appropriate Canvas module (in the form of an attachment to a "Discussion" post) prior to 5:00 pm CDT today. You need only submit a **single** file/script to be considered for credit (i.e., this .Rmd with your code inserted below each question). Recall, I don't care whether you got things right. I only care that attempts to each question have been made._ 

We will now walk through some of the techniques covered this week and last, but this time using real data. Specifically, for this set of exercises, you will use the 2016 American National Election Pilot Study (ANES). Load the data:

```{r echo = TRUE, eval = FALSE}
library(tidyverse)
library(here)

anes <- read_csv(here("data", "anes_2016.csv"))
```

With the data loaded, answer the following questions. The objective here is twofold: 1) to practice your statistical computing skills, and 2) apply and explore error from fitting models on different sets of data.

1. Using some of the techniques we covered last week:

    a. Select only the Obama feeling thermometer (`ftobama`), household income (`faminc`), party affiliation on a 3 point scale (`pid3`), birth year (`birthyr`), and gender (`gender`) (*be sure to recode missing values to `NA` and omit these*)
  
    b. Split the subset data into training (75%) and testing (25%) sets (*hint*: remember to set the seed (`set.seed()`) prior to creating the split, as the proportions are generated at random)
  
    c. Plot the distributions of each against each other to ensure they look similar

```{r echo = FALSE, eval = FALSE}
# a. 
anes_sub <- anes %>%
  select(ftobama, faminc, pid3, birthyr, gender) %>% 
  mutate(ftobama = replace(ftobama, ftobama > 100, NA)) %>% 
  drop_na()

# b. 
library(tidymodels)

set.seed(1234)

split <- initial_split(anes_sub)
train <- training(split)
test  <- testing(split)

# c.  
train %>% 
  ggplot(aes(x = ftobama)) + 
  geom_line(stat = "density") + 
  geom_line(data = test, 
            stat = "density", 
            col = "red") +
  labs(title = "Comparing Training and Testing Distributions") +
  theme_minimal()

```

2. Fit a linear regression (`lm()`) on the *training* data, predicting obama approval as a function of all other features.

```{r echo = FALSE, eval = FALSE}
linear_model <- lm(ftobama ~ ., train)
```

3. Calculate the training mean squared error (*hint*: consider using the `mse()` function from Dr. Soltoff's `rcfss` package, which is at the uc-cfss github, *not* on CRAN).

```{r echo = FALSE, eval = FALSE}
library(broom)
library(rcfss)

train_mse <- augment(linear_model) %>%
  mse(truth = ftobama, 
      estimate = .fitted)
```

4. Calculate predictions for the testing set, using the model you built on the training set (*hint*: consider either `predict()` from base R, or `augment()` from `broom`).

```{r echo = FALSE, eval = FALSE}
predict(linear_model, test) %>% 
  tibble()
```

5. Calculate the testing mean squared error.

```{r echo = FALSE, eval = FALSE}
test_mse <- augment(linear_model, 
                    newdata = test) %>%
    mse(truth = ftobama, 
        estimate = .fitted)
```

6. Compare the mean squared error from both sets numerically, side-by-side. What do you see? *Discuss in your groups and record a few sentences as a response.*

```{r echo = FALSE, eval = FALSE}
mse <- cbind(train_mse$.estimate, 
     test_mse$.estimate)

colnames(mse) <- c("Train MSE", "Test MSE"); mse
```

7. Write your own function to calculate the MSE. Then, use it to re-answer questions 3 and 5. Present the results here, and compare with the `rcfss` approach via `mse()`. These results should be identical to the `mse()` version. Are they? If not, *why* do you think? (*just a sentence or two on your general thoughts if they differ*) 

```{r echo = FALSE, eval = FALSE}
# write the function (look back at the lecture notes if need be)
my_mse <- function(hat_y, y) {
  mean((hat_y - y) ^ 2)
}

# training fitted vals, then training mse
train_preds <- augment(linear_model)

my_mse(train_preds$.fitted, train$ftobama)

# test preds, then test mse
test_preds <- augment(linear_model, 
                      newdata = test)

my_mse(test_preds$.fitted, test$ftobama)
```

